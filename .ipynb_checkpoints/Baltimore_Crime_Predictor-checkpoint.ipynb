{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipydeps\n",
    "ipydeps.pip(['requests','tqdm','plotly','tensorflow'])\n",
    "import IPython\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import QuantileTransformer, OrdinalEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "def days_hours_minutes(td):\n",
    "    return td.days, td.seconds//3600, (td.seconds//60)%60\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6caef0",
   "metadata": {},
   "source": [
    "# Initializing Baltimore Data Extraction class\n",
    "- The API link dictionary organizes each data set by type of data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bmore_API_Crime_link_dict = {'FEAT_COLLECT':\n",
    "                             {#'911 Calls for Service 2022 Through Present':'https://services1.arcgis.com/UWYHeuuJISiGmgXx/ArcGIS/rest/services/911_CallsForService_PreviousYear_Present/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                              #'BPD Arrests':'https://egis.baltimorecity.gov/egis/rest/services/GeoSpatialized_Tables/Arrest/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                              #'Parking and Moving Citations':'https://services1.arcgis.com/UWYHeuuJISiGmgXx/arcgis/rest/services/Parking_and_Moving_Citations__view/FeatureServer/2/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                              #'Gun Offenders Registry':'https://egis.baltimorecity.gov/egis/rest/services/GeoSpatialized_Tables/GunOffenders/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                              'Part 1 Crime Data':'https://services1.arcgis.com/UWYHeuuJISiGmgXx/arcgis/rest/services/Part1_Crime_Beta/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson'},\n",
    "                             'CSA_DATA':\n",
    "                             {'Percent of Adult Population on Parole/Probation - Community Statistical Area':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Prbprl/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                              'Property Crime Rate per 1,000 Residents - Community Statistical Area':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Prop/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                              'Violent Crime Rate per 1,000 Residents - Community Statistical Area':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Viol/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                              'Part 1 Crime Rate per 1,000 Residents - Community Statistical Area':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Crime/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                              'Number of Gun-Related Homicides per 1,000 Residents - Community Statistical Area':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Gunhom/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                              'Number of Arrests per 1,000 Residents - Community Statistical Area':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Arrests/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                              'Domestic Violence Calls For Service per 1,000 Residents - Community Statistical Area':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Juvarr/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                              'Juvenile Arrest Rate per 1,000 Juveniles - Community Statistical Area':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Juvarr/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                              'Juvenile Arrest Rate for Violent Offenses per 1,000 Juveniles - Community Statistical Area':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Juvviol/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                              'Juvenile Arrest Rate for Drug-Related Offenses per 1,000 Juveniles - Community Statistical Area':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Juvdrug/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                              'Number of Narcotics Calls for Service per 1,000 Residents - Community Statistical Area':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Narc/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson'},\n",
    "                             'CITY_DATA':\n",
    "                             {'Property Crime Rate per 1,000 Residents - City':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Prop/FeatureServer/1/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                             'Violent Crime Rate per 1,000 Residents - City':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Viol/FeatureServer/1/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                             'Part 1 Crime Rate per 1,000 Residents - City':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Crime/FeatureServer/1/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                             'Number of Gun-Related Homicides per 1,000 Residents - City':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Gunhom/FeatureServer/1/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                             'Number of Narcotics Calls for Service per 1,000 Residents - City':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Narc/FeatureServer/1/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                             'Number of Common Assault Calls for Service per 1,000 Residents - City':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Caslt/FeatureServer/1/query?outFields=*&where=1%3D1&f=geojson',\n",
    "                             'Number of Common Assault Calls for Service per 1,000 Residents - City':'https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/Caslt/FeatureServer/1/query?outFields=*&where=1%3D1&f=geojson'}\n",
    "                            }\n",
    "\n",
    "with open(\"Bmore_API_Crime_link.json\", \"w\") as outfile:\n",
    "    json.dump(Bmore_API_Crime_link_dict, outfile, indent=4)\n",
    "\n",
    "class BmoreDataExtraction:\n",
    "    \"\"\"\n",
    "    This class is meant to download a large number of JSON files based on an \n",
    "    input dictionary that contains API links to Baltimore's Open Data.\n",
    "    \n",
    "    The input dictionary must be in the following format:\n",
    "        {'DATA_TYPE_1':\n",
    "            {'DATA_NAME_A':'API_LINK_A',\n",
    "             'DATA_NAME_B':'API_LINK_B',\n",
    "             'DATA_NAME_C':'API_LINK_C'},\n",
    "         'DATA_TYPE_2':\n",
    "            {'DATA_NAME_A':'API_LINK_A',\n",
    "             'DATA_NAME_B':'API_LINK_B',\n",
    "             'DATA_NAME_C':'API_LINK_C'}\n",
    "        }\n",
    "    Example:\n",
    "        {'TABULAR_DATA':\n",
    "            {'Part 1 Crime Data':'https://opendata.baltimorecity.gov/egis/rest/ ...so on so forth'}}\n",
    "\n",
    "    Baltimore's Open Data uses arcgis to store most (if not all) of their \n",
    "    data. This class can also help you change multiple storage formats of \n",
    "    baltimore data into a pandas DataFrame for other uses.\n",
    "    \"\"\"\n",
    "    def __init__(self,API_dict):\n",
    "        self.API_dict_ = API_dict\n",
    "        self.data_types_ = list(API_dict.keys())\n",
    "    \n",
    "    def CleanDataNames(self,some_list):\n",
    "        clean_list = [dataName.replace(' ','_').replace(',','').replace('-','').replace('/','_')\n",
    "                      for dataName in some_list]\n",
    "        return clean_list\n",
    "    \n",
    "    def LinkCheck(self,data_type_str):\n",
    "        HTTP_Status_dict = {data_name:requests.get(self.API_dict_[data_type_str][data_name]).status_code \n",
    "                            for data_name in self.API_dict_[data_type_str].keys()}\n",
    "        available_li = [name if HTTP_Status_dict[name] == 200 else None \n",
    "                        for name in HTTP_Status_dict.keys()]\n",
    "        return HTTP_Status_dict, available_li\n",
    "\n",
    "    def createDataPath(self,existing_dir=None):\n",
    "        cwd = os.getcwd()\n",
    "        if existing_dir is None:\n",
    "            Bmore_data_path = cwd+'/Baltimore_Data/'\n",
    "            return Bmore_data_path \n",
    "        else:\n",
    "            Bmore_data_path = cwd+'/'+existing_dir+'/Baltimore_Data/'\n",
    "            return Bmore_data_path\n",
    "    \n",
    "    def DownloadDataType(self,data_type_str,chosen_dir=None):\n",
    "        HTTP_Status_dict, available_li = self.LinkCheck(data_type_str)\n",
    "        clean_available_li = self.CleanDataNames(available_li)\n",
    "        OGName_CleanName_dict = dict(zip(available_li,clean_available_li))\n",
    "        print('Downloading Available Baltimore Data...')\n",
    "        if chosen_dir is None:\n",
    "            path_str = self.createDataPath()\n",
    "        if chosen_dir is not None:\n",
    "            path_str = self.createDataPath(chosen_dir)\n",
    "        if os.path.exists(path_str) is False:\n",
    "            os.mkdir(path_str)\n",
    "        DataPath_dict = {}\n",
    "        for data_name in tqdm(available_li):\n",
    "            filename_str = OGName_CleanName_dict[data_name]+'_'+str(date.today())+'.json'\n",
    "            full_path_str = path_str+filename_str\n",
    "            r = requests.get(self.API_dict_[data_type_str][data_name])\n",
    "            with open(path_str+filename_str, \"w+\") as f:\n",
    "                json.dump(r.json(), f)\n",
    "            DataPath_dict[data_name] = path_str+filename_str\n",
    "        return DataPath_dict\n",
    "    \n",
    "    def FeatureCollectionToDF(self,DataPath_dict):\n",
    "        print('Turning Feature Collections data Into DataFrames...')\n",
    "        FEAT_DATA_DF_dict = {}\n",
    "        for key in tqdm(DataPath_dict.keys()):\n",
    "            f = open(DataPath_dict[key])\n",
    "            oneTableData_dict = json.load(f)\n",
    "            FEAT_DATA_DF_dict[key] = self.ExtractAllProperties(oneTableData_dict)\n",
    "        return FEAT_DATA_DF_dict\n",
    "        \n",
    "    def GenerateMeanGeoDict(self,lon_lat_ar):\n",
    "        # This function expects an input of a 2d numpy array where the first\n",
    "        # column is the Longitude and the second column is the Latitude.\n",
    "        mean_geo_ar = np.mean(lon_lat_ar,axis=0)\n",
    "        std_geo_ar = np.std(lon_lat_ar,axis=0)\n",
    "        row_dict = {'MEAN_LAT':[mean_geo_ar[1]],\n",
    "                    'MEAN_LON':[mean_geo_ar[0]],\n",
    "                    'STD_LAT':[std_geo_ar[1]],\n",
    "                    'STD_LON':[std_geo_ar[0]]}\n",
    "        return row_dict\n",
    "    \n",
    "    def ExtractAllProperties(self,oneDataFile_dict):\n",
    "        try:\n",
    "            row_li = [sub_dict['properties'] for sub_dict in oneDataFile_dict['features']]\n",
    "            return pd.DataFrame(row_li)\n",
    "        except KeyError:\n",
    "            print('Successful connection, but no data.')\n",
    "            return pd.DataFrame([0])\n",
    "        \n",
    "    \n",
    "    def ExtractAllCoordinates(self,oneDataFile_dict):\n",
    "        AREAgeo_ar_li = [np.array(AREA['geometry']['coordinates'][0]) \n",
    "                        for AREA in oneDataFile_dict['features']]\n",
    "        GeoStatFrames_li = []\n",
    "        for ar in AREAgeo_ar_li:\n",
    "            # check to see if shape is length of 2 \n",
    "            # then generate means and standard deviations.\n",
    "            if len(ar.shape) > 2:\n",
    "                bad_shape_tu = ar.shape\n",
    "                new_shape_tu = (bad_shape_tu[1],bad_shape_tu[2])\n",
    "                new_ar = np.reshape(ar,new_shape_tu)\n",
    "                new_row_dict = self.GenerateMeanGeoDict(new_ar)\n",
    "                GeoStatFrames_li.append(pd.DataFrame(new_row_dict))\n",
    "            else:\n",
    "                row_dict = self.GenerateMeanGeoDict(ar)\n",
    "                GeoStatFrames_li.append(pd.DataFrame(row_dict))\n",
    "        meanGeos_df = pd.concat(GeoStatFrames_li).reset_index(drop=True)\n",
    "        return meanGeos_df\n",
    "\n",
    "    def GeoDataToDF(self,DataPath_dict):\n",
    "        # This function can extract all the \"Community Statistical Area\" (CSA) data \n",
    "        # and City level data specified in your API link dictionary.\n",
    "        print('Turning Geo JSON data into DataFrames...')\n",
    "        GEO_DATA_DF_dict = {}\n",
    "        for key in tqdm(DataPath_dict.keys()):\n",
    "            f = open(DataPath_dict[key])\n",
    "            oneGeoData_dict = json.load(f)\n",
    "            # This DataFrame contains the properties of a given Community Statistical Area (CSA)\n",
    "            properties_df = self.ExtractAllProperties(oneGeoData_dict)\n",
    "            # This DataFrame contains the mean latitudes and mean longitudes of a given\n",
    "            # Community Statistical Area (CSA)\n",
    "            meanGeos_df = self.ExtractAllCoordinates(oneGeoData_dict)\n",
    "            GEO_DATA_df = properties_df.merge(meanGeos_df,left_index=True,right_index=True)\n",
    "            GEO_DATA_DF_dict[key] = GEO_DATA_df\n",
    "        return GEO_DATA_DF_dict\n",
    "\n",
    "bde = BmoreDataExtraction(Bmore_API_Crime_link_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c0cd7",
   "metadata": {},
   "source": [
    "# Downloading latest data set by type of data\n",
    "- next 3 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb180c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FEAT_COLLECT_Path_dict = bde.DownloadDataType('FEAT_COLLECT')\n",
    "FEAT_COLLECT_DF_dict = bde.FeatureCollectionToDF(FEAT_COLLECT_Path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSA_DATA_Path_dict = bde.DownloadDataType('CSA_DATA')#,'pyCodes')\n",
    "CSA_DATA_DF_dict = bde.GeoDataToDF(CSA_DATA_Path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf24f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY_DATA_Path_dict = bde.DownloadDataType('CITY_DATA')#,'pyCodes')\n",
    "CITY_DATA_DF_dict = bde.GeoDataToDF(CITY_DATA_Path_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63449550",
   "metadata": {},
   "source": [
    "# Cleaning \"Part 1 Crime\" Data Specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9baef1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "crime_feat_dict = {'DROP':\n",
    "                        ['RowID',\n",
    "                         'Post',\n",
    "                         'Gender',\n",
    "                         'Age',\n",
    "                         'Race',\n",
    "                         'Ethnicity',\n",
    "                         'GeoLocation',\n",
    "                         'PremiseType',\n",
    "                         'Total_Incidents'],\n",
    "                   'CATEGORICAL':\n",
    "                         ['CCNumber',\n",
    "                          'CrimeCode',\n",
    "                          'Description',\n",
    "                          'Inside_Outside',\n",
    "                          'Weapon',\n",
    "                          'Location',\n",
    "                          'Old_District',\n",
    "                          'New_District',\n",
    "                          'Neighborhood'],\n",
    "                    'NUMERICAL':\n",
    "                         []}\n",
    "\n",
    "class BmoreCrimeFeatureEngineering:\n",
    "    def __init__(self,geo_cols_li,time_col_str,feat_col_dict):\n",
    "        self.geo_cols_ = geo_cols_li\n",
    "        self.time_col_ = time_col_str\n",
    "        self.feature_cols_ = feat_col_dict\n",
    "        self.day_ = 24*60*60\n",
    "        self.year_ = (365.2425)*self.day_\n",
    "        \n",
    "    def CreateTimeIndexDF(self,df):\n",
    "        new_df = df.copy()\n",
    "        datetime_se = new_df[self.time_col_].astype(float).div(1000.0).apply(datetime.utcfromtimestamp)\n",
    "        new_df.index = datetime_se.rename('DATETIME')\n",
    "        new_df = new_df.drop_duplicates()\n",
    "        new_df = new_df.sort_index(ascending=False).drop_duplicates()\n",
    "        return new_df\n",
    "    \n",
    "    def DropUnwantedFeatsDF(self,df):\n",
    "        new_df = df.copy()\n",
    "        new_df = new_df.drop(columns=self.feature_cols_['DROP']).fillna('UKNOWN')\n",
    "        new_df = new_df[new_df[self.geo_cols_[0]] != 'UKNOWN']\n",
    "        return new_df\n",
    "    \n",
    "    def GeoStatsDF(self,df):\n",
    "        df['LAT'] = df[self.geo_cols_[0]].astype('float')\n",
    "        df['LON'] = df[self.geo_cols_[1]].astype('float')\n",
    "        df['LAT_STDEV'] = df['LAT'].std()\n",
    "        df['LON_STDEV'] = df['LON'].std()\n",
    "        df = df.drop_duplicates()\n",
    "        return df\n",
    "        \n",
    "    def CatFeatEncode(self,df):\n",
    "        oe = OrdinalEncoder()\n",
    "        Cat_X = df[self.feature_cols_['CATEGORICAL']].values\n",
    "        E_Cat_df = pd.DataFrame(oe.fit_transform(Cat_X),\n",
    "                                columns=self.feature_cols_['CATEGORICAL'],\n",
    "                                index=df.index)\n",
    "        return E_Cat_df\n",
    "\n",
    "    def PeriodicityEncode(self,df):\n",
    "        new_df = self.CreateTimeIndexDF(df)\n",
    "        # turning pandas datetime to seconds\n",
    "        new_df['seconds'] = new_df.index.map(pd.Timestamp.timestamp)\n",
    "        # hourly frequency of events in radians\n",
    "        new_df['Day sin'] = np.sin(new_df['seconds'] * (2 * np.pi / self.day_))\n",
    "        new_df['Day cos'] = np.cos(new_df['seconds'] * (2 * np.pi / self.day_))\n",
    "        # yearly frequency of events in radians\n",
    "        new_df['Year sin'] = np.sin(new_df['seconds'] * (2 * np.pi / self.year_))\n",
    "        new_df['Year cos'] = np.cos(new_df['seconds'] * (2 * np.pi / self.year_))\n",
    "        new_df = new_df[['Day sin','Day cos','Year sin','Year cos']]\n",
    "        return new_df\n",
    "        \n",
    "    def DropZeroGeos(self,df):\n",
    "        float_df = df[self.geo_cols_].astype(float)\n",
    "        initial_len = len(float_df)\n",
    "        clean_df = df[float_df[self.geo_cols_[0]] != 0.0]\n",
    "        clean_len = len(clean_df)\n",
    "        print(str(initial_len-clean_len),'data points lack geos')\n",
    "        return clean_df\n",
    "\n",
    "# instantiating feature engineering class\n",
    "bcfe = BmoreCrimeFeatureEngineering(geo_cols_li=['Latitude',\n",
    "                                                 'Longitude'],\n",
    "                                    time_col_str='CrimeDateTime',\n",
    "                                    feat_col_dict=crime_feat_dict)\n",
    "\n",
    "# Initial Cleaning of Baltimore crime data\n",
    "Bmore_Crime_df = bcfe.CreateTimeIndexDF(FEAT_COLLECT_DF_dict['Part 1 Crime Data'])\n",
    "Bmore_Crime_df = bcfe.DropUnwantedFeatsDF(Bmore_Crime_df)\n",
    "Bmore_Crime_df = bcfe.GeoStatsDF(Bmore_Crime_df)\n",
    "print('Bmore Crime Shape:',Bmore_Crime_df.shape)\n",
    "\n",
    "# Extracting categorical and periodic features\n",
    "categorical_df = bcfe.CatFeatEncode(Bmore_Crime_df)\n",
    "periodicity_df = bcfe.PeriodicityEncode(Bmore_Crime_df)\n",
    "print('Categorical Feature Shape:',categorical_df.shape)\n",
    "print('periodicity Feature Shape:',periodicity_df.shape)\n",
    "\n",
    "# Merging features & scaling\n",
    "FINAL_X_df = pd.concat([categorical_df,periodicity_df],axis=1)\n",
    "print('Feature Shape (X):',FINAL_X_df.shape)\n",
    "# Separating Prediction targets\n",
    "FINAL_Y_df = Bmore_Crime_df[bcfe.geo_cols_].apply(pd.to_numeric)\n",
    "print('Target Shape (Y):',FINAL_Y_df.shape)\n",
    "\n",
    "# Combining everything together again\n",
    "FINAL_DF = pd.concat([FINAL_X_df,FINAL_Y_df],axis=1).sort_index()\n",
    "FINAL_DF = bcfe.DropZeroGeos(FINAL_DF)\n",
    "print('Final Shape Regular (X+Y):',FINAL_DF.shape)\n",
    "#resampling  for every hour\n",
    "resampled_FINAL_DF = FINAL_DF.resample(rule='1H').mean(numeric_only=True).interpolate(method='linear',axis=0)\n",
    "resampled_FINAL_shape = resampled_FINAL_DF.shape\n",
    "print('Final Shape Resampled (X+Y):',resampled_FINAL_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0794b1",
   "metadata": {},
   "source": [
    "# Splitting Data Into Train, Validation, & Test Sets\n",
    "- Normalizing data based on features in training set. Using quantile transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c690aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def DataSplitter(df):\n",
    "    # Splitting training, validation, and test sets\n",
    "    column_indices_dict = {name: i for i, name in enumerate(df.columns)}\n",
    "    n = len(df)\n",
    "    TRAIN_DF = df[0:int(n*0.7)]\n",
    "    VAL_DF = df[int(n*0.7):int(n*0.9)]\n",
    "    TEST_DF = df[int(n*0.9):]\n",
    "    print('Train Shape:',TRAIN_DF.shape)\n",
    "    print('Train Datetime Range:',\n",
    "          str(TRAIN_DF.index.min())+' -> '+str(TRAIN_DF.index.max())+'\\n')\n",
    "    print('Validation Shape:',VAL_DF.shape)\n",
    "    print('Validation Datetime Range:',\n",
    "          str(VAL_DF.index.min())+' -> '+str(VAL_DF.index.max())+'\\n')\n",
    "    print('Test Shape:',TEST_DF.shape)\n",
    "    print('Test Datetime Range:',\n",
    "          str(TEST_DF.index.min())+' -> '+str(TEST_DF.index.max())+'\\n')\n",
    "    return {'TRAIN':TRAIN_DF,'VAL':VAL_DF,'TEST':TEST_DF}\n",
    "\n",
    "split_dict = DataSplitter(resampled_FINAL_DF)\n",
    "\n",
    "def NormalizeSplits(split_dict=split_dict):\n",
    "    # Normalizing data sets via L2\n",
    "    # Fitting to the training set\n",
    "    norm = QuantileTransformer().fit(split_dict['TRAIN'])\n",
    "    # Transforming all the split sets\n",
    "    scaled_TRAIN_DF = pd.DataFrame(norm.transform(split_dict['TRAIN']),\n",
    "                                   columns=norm.get_feature_names_out(),\n",
    "                                   index=split_dict['TRAIN'].index)\n",
    "    scaled_VAL_DF = pd.DataFrame(norm.transform(split_dict['VAL']),\n",
    "                                   columns=norm.get_feature_names_out(),\n",
    "                                   index=split_dict['VAL'].index)\n",
    "    scaled_TEST_DF = pd.DataFrame(norm.transform(split_dict['TEST']),\n",
    "                                    columns=norm.get_feature_names_out(),\n",
    "                                    index=split_dict['TEST'].index)\n",
    "    return norm, {'TRAIN':scaled_TRAIN_DF,'VAL':scaled_VAL_DF,'TEST':scaled_TEST_DF}\n",
    "\n",
    "norm, scaled_split_dict = NormalizeSplits(split_dict)\n",
    "\n",
    "# Creating violin plot to show data features\n",
    "voilin_df = scaled_split_dict['TRAIN'].melt(var_name='Column', value_name='Normalized')\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=voilin_df)\n",
    "_ = ax.set_xticklabels(list(scaled_split_dict['TRAIN'].columns), rotation=90)\n",
    "plt.title('Training Data Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ebdf2",
   "metadata": {},
   "source": [
    "# Data Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bddb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self,\n",
    "                 input_width,\n",
    "                 label_width,\n",
    "                 shift,\n",
    "                 split_dict,\n",
    "                 label_columns=None):\n",
    "        # Store the raw data.\n",
    "        self.train_df = split_dict['TRAIN']\n",
    "        self.val_df = split_dict['VAL']\n",
    "        self.test_df = split_dict['TEST']\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is None:\n",
    "            self.column_indices = {name: i for i, name in\n",
    "                                   enumerate(split_dict['TRAIN'].columns)}\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                          enumerate(label_columns)}\n",
    "            self.column_indices = {name: i for i, name in\n",
    "                                   enumerate(split_dict['TRAIN'].columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        \n",
    "        self.total_window_size = input_width + shift\n",
    "        \n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "        \n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([f'Total window size: {self.total_window_size}',\n",
    "                          f'Input indices: {self.input_indices}',\n",
    "                          f'Label indices: {self.label_indices}',\n",
    "                          f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "            axis=-1)\n",
    "            \n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "        return inputs, labels\n",
    "    \n",
    "    def plot(self, model=None, plot_col=None, max_subplots=3):\n",
    "        inputs, labels = self.example\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plot_col_index = self.column_indices[plot_col]\n",
    "        max_n = min(max_subplots, len(inputs))\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(max_n, 1, n+1)\n",
    "            plt.ylabel(f'{plot_col} [normed]')\n",
    "            plt.plot(self.input_indices,\n",
    "                     inputs[n, :, plot_col_index],\n",
    "                     label='Inputs', marker='.', zorder=-10)\n",
    "            if self.label_columns:\n",
    "                label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            else:\n",
    "                label_col_index = plot_col_index\n",
    "                \n",
    "            if label_col_index is None:\n",
    "                continue\n",
    "                \n",
    "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "            if n == 0:\n",
    "                plt.legend()\n",
    "            \n",
    "            plt.xlabel('Time [h]')\n",
    "    \n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(data=data,\n",
    "                                                          targets=None,\n",
    "                                                          sequence_length=self.total_window_size,\n",
    "                                                          sequence_stride=1,\n",
    "                                                          shuffle=True,\n",
    "                                                          batch_size=32)\n",
    "        ds = ds.map(self.split_window)\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result\n",
    "\n",
    "## Creating a multistep output to predict more that one window at a time\n",
    "OUT_STEPS = 168*2 # Each time step is 1 hour. 168 hours in a week\n",
    "INPUT_STEPS = 730*2 # Each time step is 1 hour. 730 hours in a month\n",
    "num_features = resampled_FINAL_shape[1]\n",
    "\n",
    "wg = WindowGenerator(input_width=INPUT_STEPS,\n",
    "                     label_width=OUT_STEPS,\n",
    "                     label_columns=None, # Originally just latitude\n",
    "                     shift=OUT_STEPS,\n",
    "                     split_dict=scaled_split_dict)\n",
    "wg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80616c1f",
   "metadata": {},
   "source": [
    "# Training LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60678af",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20\n",
    "multi_val_performance = {}\n",
    "multi_performance = {}\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "    # Created using M1/M2 Mac hence the use of \"tf.keras.optimizers.legacy.Adam\" instead of \"tf.keras.optimizers.Adam\"\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(), \n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "    return history\n",
    "\n",
    "multi_lstm_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units].\n",
    "    # Adding more `lstm_units` just overfits more quickly.\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    # Shape => [batch, out_steps*features].\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros()),\n",
    "    # Shape => [batch, out_steps, features].\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "train_start = datetime.now()\n",
    "history = compile_and_fit(multi_lstm_model, wg)\n",
    "IPython.display.clear_output()\n",
    "train_finish = datetime.now() - train_start\n",
    "print('Total Training Time {d}:{h}:{m}'.format(d=days_hours_minutes(train_finish)[0],\n",
    "                                               h=days_hours_minutes(train_finish)[1],\n",
    "                                               m=days_hours_minutes(train_finish)[2]))\n",
    "multi_val_performance['LSTM'] = multi_lstm_model.evaluate(wg.val)\n",
    "multi_performance['LSTM'] = multi_lstm_model.evaluate(wg.test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d56438",
   "metadata": {},
   "source": [
    "# Plotting Fit Examples From Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1246f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg.plot(multi_lstm_model,plot_col=bcfe.geo_cols_[0])\n",
    "wg.plot(multi_lstm_model,plot_col=bcfe.geo_cols_[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114ee4a",
   "metadata": {},
   "source": [
    "# LSTM Inference On Latest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748915af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Displaying metadata about model \n",
    "print('Input Shape:',multi_lstm_model.input_shape)\n",
    "print('Output Shape:',multi_lstm_model.output_shape)\n",
    "display(multi_lstm_model.summary())\n",
    "\n",
    "## Getting the latest crime data and preparing for inference\n",
    "deploy_df = wg.test_df.tail(INPUT_STEPS)\n",
    "deploy_shape_tu = deploy_df.shape\n",
    "deploy_ar = np.reshape(deploy_df.values,newshape=(1,deploy_shape_tu[0],deploy_shape_tu[1]))\n",
    "\n",
    "## Obtaining forecasted datetime range\n",
    "LATEST_DATETIME = deploy_df.tail(1).index[0]\n",
    "FORECAST_DATE_RANGE = pd.date_range(start=LATEST_DATETIME,\n",
    "                                    freq='1H',\n",
    "                                    periods=OUT_STEPS+1)[1:]\n",
    "\n",
    "## Generating inference and scaling back to original data\n",
    "forecast_ar = multi_lstm_model.predict(deploy_ar)\n",
    "forecast_df = pd.DataFrame(norm.inverse_transform(forecast_ar[0]),\n",
    "                           columns=wg.column_indices,\n",
    "                           index=FORECAST_DATE_RANGE)\n",
    "geo_forecast_df = forecast_df[bcfe.geo_cols_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae16ae",
   "metadata": {},
   "source": [
    "# Visualizing Predicted Crime Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e975b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.density_mapbox(geo_forecast_df,\n",
    "                        lat=bcfe.geo_cols_[0],\n",
    "                        lon=bcfe.geo_cols_[1],\n",
    "                        mapbox_style=\"stamen-terrain\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844630aa",
   "metadata": {},
   "source": [
    "# Calculating different forms of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926961f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_sigma_se = FINAL_DF.describe().loc['std']\n",
    "lat_error = geo_sigma_se[bcfe.geo_cols_[0]]\n",
    "lon_error = geo_sigma_se[bcfe.geo_cols_[1]]\n",
    "train_error = multi_val_performance['LSTM'][1]\n",
    "test_error = multi_performance['LSTM'][1]\n",
    "\n",
    "geo_uncertainty = math.sqrt((lat_error**2)+(lon_error**2))\n",
    "model_error = math.sqrt((train_error**2)+(test_error**2))\n",
    "combined_uncertainty = math.sqrt((lat_error**2)+(lon_error**2)+(train_error**2)+(test_error**2))\n",
    "print('Geo Uncertainty:',geo_uncertainty)\n",
    "print('Model Error:',model_error)\n",
    "print('Combined Uncertainty:',combined_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff884ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5bb4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1699c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
